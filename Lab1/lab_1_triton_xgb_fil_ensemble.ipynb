{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e386477",
   "metadata": {},
   "source": [
    "# Pre-processing and XGBoost model inference pipeline with NVIDIA Triton Inference Server on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6713c",
   "metadata": {},
   "source": [
    "This notebooks makes use of a pre-trained XGBoost model for a Fraud Detection use-case. The notebook can be found [here](https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-triton/fil_ensemble/1_prep_rapids_train_xgb.ipynb) which showcases how the model was trained. Note: you do not need to run this model as the trained model artifact has been provided in this repository. \n",
    "\n",
    "With the 22.05 version release of [NVIDIA Triton](https://github.com/triton-inference-server/server/) container image on SageMaker you can now use Triton's Forest Inference Library (FIL) backend to easily serve tree based ML models like XGBoost for high-performance CPU and GPU inference in SageMaker. Using Triton's FIL backend allows you to benefit from performance optimizations like dynamic batching and concurrent execution which help maximize the utilization of GPU and CPU, further lowering the cost of inference. The multi-framework support provided by NVIDIA Triton allows you to seamlessly deploy tree-based ML models alongside deep learning models for fast, unified inference pipelines.\n",
    "\n",
    "Machine Learning applications are complex and can often require data pre-processing. In this notebook, we will not only deep dive into how to deploy a tree-based ML model like XGBoost using the FIL Backend in Triton on SageMaker endpoint but also cover how to implement python-based data pre-processing inference pipeline for your model using the ensemble feature in Triton. This will allow us to send in the raw data from client side and have both data pre-processing and model inference happen in Triton SageMaker endpoint for the optimal inference performance.\n",
    "\n",
    "## To Run This Notebook Please Select `Python 3 (Data Science)` Kernel from the Kernel Dropdown menu\n",
    "\n",
    "**Note:** This notebook was tested with the `Python 3 (Data Science)` kernel on an Amazon SageMaker Studio instance of type ` `ml.c5.xlarge`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9fc77d",
   "metadata": {},
   "source": [
    "## Forest Inference Library (FIL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9686e34",
   "metadata": {},
   "source": [
    "RAPIDS Forest Inference Library (FIL) is a library to provide high-performance inference for tree-based models. Here are some important FIL features:\n",
    "\n",
    "* Supports XGBoost, LightGBM, cuML RandomForest, and Scikit Learn Random Forest\n",
    "* No conversion needed for XGBoost and LightGBM. SKLearn or cuML pickle models need to be converted to Treelite's binary checkpoint format \n",
    "* SKLearn Random Forest is supported for single-output regression and multi-class classification\n",
    "* Both CPU and GPU are supported\n",
    "\n",
    "Below we show benchmark highlighting FIL's throughput performance against CPU XGBoost.\n",
    "\n",
    "<img src=\"./images/fil_benchmark.png\" alt=\"fil-benchmark\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7b53e",
   "metadata": {},
   "source": [
    "## Triton FIL Backend\n",
    "FIL is available as a backend in Triton with features to allow for serving XGBoost, LightGBM and RandomForest models both on CPU and GPU with high performance. Here are some important features of the FIL Backend:\n",
    "\n",
    "* **Shapley Value Support (GPU)**: GPU Shapley Values are supported for Model Explainability\n",
    "* **Categorical Feature Support**: Models trained on categorical features fully supported.\n",
    "* **CPU Optimizations**: Optimized CPU mode offers faster execution than native XGBoost.\n",
    "\n",
    "To learn more about FIL Backend's features please see the [FAQ Notebook](https://github.com/triton-inference-server/fil_backend/blob/fea-faq_nb/notebooks/faq/FAQs.ipynb) and [Triton FIL Backend GitHub.](https://github.com/triton-inference-server/fil_backend/tree/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3907aeb3",
   "metadata": {},
   "source": [
    "## Triton Model Ensemble Feature\n",
    "Triton Inference Server greatly simplifies the deployment of AI models at scale in production. Triton Server comes with a convenient solution that simplifies building pre-processing and post-processing pipelines. Triton Server platform provides the ensemble scheduler, which is responsible for pipelining models participating in the inference process while ensuring efficiency and optimizing throughput. Using ensemble models can avoid the overhead of transferring intermediate tensors and minimize the number of requests that must be sent to Triton.\n",
    "\n",
    "<img src=\"./images/triton-ensemble.png\" alt=\"triton-ensemble\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade5d7a",
   "metadata": {},
   "source": [
    "In this notebook we will be show how to use the ensemble feature for building a pipeline of data preprocessing with XGBoost model inference and you can extrapolate from it to add custom postprocessing to the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32d93c",
   "metadata": {},
   "source": [
    "## Set up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da60ff17",
   "metadata": {},
   "source": [
    "We begin by setting up the required environment. We will install the dependencies required to package our model pipeline and run inferences using Triton server. Also define the IAM role that will give SageMaker access to the model artifacts and the NVIDIA Triton ECR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a83ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyring is skipped due to an exception: 'keyring.backends'\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nvidia-pyindex in /opt/conda/lib/python3.7/site-packages (1.0.9)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mKeyring is skipped due to an exception: 'keyring.backends'\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tritonclient[http] in /opt/conda/lib/python3.7/site-packages (2.29.0)\n",
      "Requirement already satisfied: python-rapidjson>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from tritonclient[http]) (1.9)\n",
      "Requirement already satisfied: numpy>=1.19.1 in /opt/conda/lib/python3.7/site-packages (from tritonclient[http]) (1.21.6)\n",
      "Requirement already satisfied: aiohttp>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from tritonclient[http]) (3.8.3)\n",
      "Requirement already satisfied: geventhttpclient<=2.0.2,>=1.4.4 in /opt/conda/lib/python3.7/site-packages (from tritonclient[http]) (2.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (2.0.4)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (0.13.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (4.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.8.1->tritonclient[http]) (4.0.2)\n",
      "Requirement already satisfied: gevent>=0.13 in /opt/conda/lib/python3.7/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[http]) (1.4.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[http]) (2022.9.24)\n",
      "Requirement already satisfied: brotli in /opt/conda/lib/python3.7/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[http]) (1.0.9)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from geventhttpclient<=2.0.2,>=1.4.4->tritonclient[http]) (1.14.0)\n",
      "Requirement already satisfied: greenlet>=0.4.14 in /opt/conda/lib/python3.7/site-packages (from gevent>=0.13->geventhttpclient<=2.0.2,>=1.4.4->tritonclient[http]) (0.4.15)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.8.1->tritonclient[http]) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81049583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "import time\n",
    "import os\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e96f98b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    \"us-east-1\": \"785573368785\",\n",
    "    \"us-east-2\": \"007439368137\",\n",
    "    \"us-west-1\": \"710691900526\",\n",
    "    \"us-west-2\": \"301217895009\",\n",
    "    \"eu-west-1\": \"802834080501\",\n",
    "    \"eu-west-2\": \"205493899709\",\n",
    "    \"eu-west-3\": \"254080097072\",\n",
    "    \"eu-north-1\": \"601324751636\",\n",
    "    \"eu-south-1\": \"966458181534\",\n",
    "    \"eu-central-1\": \"746233611703\",\n",
    "    \"ap-east-1\": \"110948597952\",\n",
    "    \"ap-south-1\": \"763008648453\",\n",
    "    \"ap-northeast-1\": \"941853720454\",\n",
    "    \"ap-northeast-2\": \"151534178276\",\n",
    "    \"ap-southeast-1\": \"324986816169\",\n",
    "    \"ap-southeast-2\": \"355873309152\",\n",
    "    \"cn-northwest-1\": \"474822919863\",\n",
    "    \"cn-north-1\": \"472730292857\",\n",
    "    \"sa-east-1\": \"756306329178\",\n",
    "    \"ca-central-1\": \"464438896020\",\n",
    "    \"me-south-1\": \"836785723513\",\n",
    "    \"af-south-1\": \"774647643957\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.05-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e796fbd9",
   "metadata": {},
   "source": [
    "## Set up pre-processing with Triton Python Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30cc9c",
   "metadata": {},
   "source": [
    "We will be using Triton's [Python Backend](https://github.com/triton-inference-server/python_backend) to perform the some tabular data preprocessing (categotical encoding) during inference time for raw data requests coming into the server. For more information to see the preprocessing that was done during training feel free to take a look at the training notebook [here](https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-triton/fil_ensemble/1_prep_rapids_train_xgb.ipynb).\n",
    "\n",
    "\n",
    "The Python backend enables pre-process, post-processing and any other custom logic to be implemented in Python and served with Triton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95bf6b",
   "metadata": {},
   "source": [
    "Using Triton on SageMaker requires us to first set up a model repository folder containing the models we want to serve. We have already set up model for python data preprocessing called `preprocessing` in the `model_repository`.\n",
    "\n",
    "<img src=\"./images/preprocessing_model.png\" alt=\"preprocessing-model\" width=\"200\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40198e8",
   "metadata": {},
   "source": [
    "Now Triton has specific requirements for model repository layout. Within the top-level model repository directory each model has its own sub-directory containing the information for the corresponding model. Each model directory in Triton must have at least one numeric sub-directory representing a version of the model. Here that is `1` representing version 1 of our python preprocessing model. Each model is executed by a specific backend so within each version sub-directory there must be the model artifact required by that backend. Here, we are using the Python backend and it requires the python file you are serving to be called `model.py` and the file needs to implement [certain functions](https://github.com/triton-inference-server/python_backend#usage). If we were using a PyTorch backend a `model.pt` file would be required and so on. For more details on naming conventions for model files please see the [model files doc](https://github.com/triton-inference-server/server/blob/185253ce225a0b012e73cade5c9a948ef9e75abd/docs/model_repository.md#model-files).\n",
    "\n",
    "\n",
    "[Our model.py](model_repository/preprocessing/1/model.py) python file we are using here implements all the tabular data preprocessing logic to convert raw data into features that can be fed into our XGBoost model.\n",
    "\n",
    "Every Triton model must also provide a `config.pbtxt` file describing the model configuration. To learn more about the config settings please see [model configuration](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md) doc. [Our config.pbtxt](model_repository/preprocessing/config.pbtxt) specifies the backend as `python` and specifies all the input columns for raw data along with preprocessed output that consists of 15 features. We also specify we want to run this python preprocessing model on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b42213e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Conda Env for Preprocessing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be5e211",
   "metadata": {},
   "source": [
    "The Python backend in Triton requires us to use conda environment for any additional dependencies. In this case we are using the Python backend to do preprocessing of the raw data before feeding it into the XGBoost model being run in FIL Backend. Even though we originally used RAPIDS cuDF and cuML to do the data preprocessing here we use Pandas and Scikit-learn as preprocessing dependencies for inference time. We do this for three reasons. \n",
    "* Firstly, to show how to create conda environment for your dependencies and how to package it in [format expected](https://github.com/triton-inference-server/python_backend#2-packaging-the-conda-environment) by Triton's Python backend. \n",
    "* Secondly, by showing the preprocessing model running in Python backend on the CPU while the XGBoost runs on the GPU in FIL Backend we illustrate how each model in Triton's ensemble pipeline can run on different framework backend as well as different hardware configurations\n",
    "* Thirdly, it highlights how the RAPIDS libraries (cuDF, cuML) are compatible with their CPU counterparts (Pandas, Scikit-learn). For example this way we get to show how LabelEncoders created in cuML can be used in Scikit-learn and vice-versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedfc6dc",
   "metadata": {},
   "source": [
    "We follow the instructions from the [Triton documentation](https://github.com/triton-inference-server/python_backend#2-packaging-the-conda-environment) for packaging preprocessing dependencies  (scikit-learn and pandas) to be used in the python backend as conda env tar file. The bash script [create_prep_env.sh](./create_prep_env.sh) creates the conda environment tar file and then we move it into the preprocessing model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aefd7687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda/envs/preprocessing_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - python=3.8\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main None\n",
      "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu None\n",
      "  ca-certificates    pkgs/main/linux-64::ca-certificates-2022.10.11-h06a4308_0 None\n",
      "  certifi            pkgs/main/linux-64::certifi-2022.12.7-py38h06a4308_0 None\n",
      "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.38-h1181459_1 None\n",
      "  libffi             pkgs/main/linux-64::libffi-3.4.2-h6a678d5_6 None\n",
      "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1 None\n",
      "  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1 None\n",
      "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1 None\n",
      "  ncurses            pkgs/main/linux-64::ncurses-6.3-h5eee18b_3 None\n",
      "  openssl            pkgs/main/linux-64::openssl-1.1.1s-h7f8727e_0 None\n",
      "  pip                pkgs/main/linux-64::pip-22.3.1-py38h06a4308_0 None\n",
      "  python             pkgs/main/linux-64::python-3.8.16-h7a1cb2a_2 None\n",
      "  readline           pkgs/main/linux-64::readline-8.2-h5eee18b_0 None\n",
      "  setuptools         pkgs/main/linux-64::setuptools-65.6.3-py38h06a4308_0 None\n",
      "  sqlite             pkgs/main/linux-64::sqlite-3.40.1-h5082296_0 None\n",
      "  tk                 pkgs/main/linux-64::tk-8.6.12-h1ccaba5_0 None\n",
      "  wheel              pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0 None\n",
      "  xz                 pkgs/main/linux-64::xz-5.2.10-h5eee18b_1 None\n",
      "  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_0 None\n",
      "\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate preprocessing_env\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 22.9.0\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda/envs/preprocessing_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - pandas\n",
      "    - scikit-learn\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  blas               conda-forge/linux-64::blas-1.1-openblas None\n",
      "  bottleneck         pkgs/main/linux-64::bottleneck-1.3.5-py38h7deecbd_0 None\n",
      "  joblib             conda-forge/noarch::joblib-1.2.0-pyhd8ed1ab_0 None\n",
      "  libblas            conda-forge/linux-64::libblas-3.9.0-15_linux64_openblas None\n",
      "  libcblas           conda-forge/linux-64::libcblas-3.9.0-15_linux64_openblas None\n",
      "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-12.2.0-h69a702a_19 None\n",
      "  libgfortran5       conda-forge/linux-64::libgfortran5-12.2.0-h337968e_19 None\n",
      "  liblapack          conda-forge/linux-64::liblapack-3.9.0-15_linux64_openblas None\n",
      "  libopenblas        conda-forge/linux-64::libopenblas-0.3.20-pthreads_h78a6416_0 None\n",
      "  numexpr            pkgs/main/linux-64::numexpr-2.8.4-py38hd2a5715_0 None\n",
      "  numpy              conda-forge/linux-64::numpy-1.22.3-py38h99721a1_2 None\n",
      "  openblas           conda-forge/linux-64::openblas-0.3.20-pthreads_h320a7e8_0 None\n",
      "  packaging          conda-forge/noarch::packaging-23.0-pyhd8ed1ab_0 None\n",
      "  pandas             pkgs/main/linux-64::pandas-1.5.2-py38h417a72b_0 None\n",
      "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.2-pyhd8ed1ab_0 None\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.8-2_cp38 None\n",
      "  pytz               conda-forge/noarch::pytz-2022.7.1-pyhd8ed1ab_0 None\n",
      "  scikit-learn       conda-forge/linux-64::scikit-learn-1.0.2-py38h1561384_0 None\n",
      "  scipy              conda-forge/linux-64::scipy-1.8.1-py38h1ee437e_0 None\n",
      "  six                conda-forge/noarch::six-1.16.0-pyh6c4a22f_0 None\n",
      "  threadpoolctl      conda-forge/noarch::threadpoolctl-3.1.0-pyh8a188c0_0 None\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2022.10.11~ --> conda-forge::ca-certificates-2022.12.7-ha878542_0 None\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/linux-64::certifi-2022.12.7~ --> conda-forge/noarch::certifi-2022.12.7-pyhd8ed1ab_0 None\n",
      "\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Retrieving notices: ...working... done\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting conda-pack\n",
      "  Downloading conda-pack-0.6.0.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m132.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/envs/preprocessing_env/lib/python3.8/site-packages (from conda-pack) (65.6.3)\n",
      "Building wheels for collected packages: conda-pack\n",
      "  Building wheel for conda-pack (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for conda-pack: filename=conda_pack-0.6.0-py2.py3-none-any.whl size=30883 sha256=5341d9b6d937669d9c6688aed2cdada3ffeda98f70bb56c5433293087ac84410\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-78ymrou_/wheels/ad/ce/cd/58348f78b175becab147bbc179095c7f9c7624f7aad28cbd6a\n",
      "Successfully built conda-pack\n",
      "Installing collected packages: conda-pack\n",
      "Successfully installed conda-pack-0.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting packages...\n",
      "Packing environment at '/opt/conda/envs/preprocessing_env' to 'preprocessing_env.tar.gz'\n",
      "[########################################] | 100% Completed | 15.2s\n"
     ]
    }
   ],
   "source": [
    "!bash create_prep_env.sh\n",
    "!cp preprocessing_env.tar.gz model_repository/preprocessing/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8232dc",
   "metadata": {},
   "source": [
    "After creating the tar file from the conda environment and placing it in model folder, you need to tell Python backend to use that environment for your model. We do this by including the lines below in the model `config.pbtxt` file:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f2be2",
   "metadata": {},
   "source": [
    "```\n",
    "parameters: {\n",
    "  key: \"EXECUTION_ENV_PATH\",\n",
    "  value: {string_value: \"$$TRITON_MODEL_DIRECTORY/preprocessing_env.tar.gz\"}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d5bcf",
   "metadata": {},
   "source": [
    "Here, `$$TRITON_MODEL_DIRECTORY` helps provide environment path relative to the model folder in model repository and is resolved to `$pwd/model_repository/preprocessing`. Finally `preprocessing_env.tar.gz` is the name we gave to our conda env file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e429da7",
   "metadata": {},
   "source": [
    "### Set up Label Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc987681",
   "metadata": {},
   "source": [
    "We also move the label encoders we had serialized to do the processing into `preprocessing` model folder so that we can use them to encode raw data categorical features at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40c9c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp label_encoders.pkl model_repository/preprocessing/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7307a",
   "metadata": {},
   "source": [
    "## Set up Tree-based ML Model for FIL Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86978d1",
   "metadata": {},
   "source": [
    "Next, we set up the model directory for tree-based ML model like XGBoost which will be using FIL Backend.\n",
    "\n",
    "The expected layout for model directory is similar to the one we showed above:\n",
    "\n",
    "<img src=\"./images/fil_model.png\" alt=\"fil-model\" width=\"200\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb89d4",
   "metadata": {},
   "source": [
    "Here, `fil` is the name of the model. We can give it a different name like xgboost if we want to. `1` is the version sub-directory which contains the model artifact, in this case it's the `xgboost.json` model that we saved at the end of [first notebook](1_prep_rapids_train_xgb.ipynb). Let's create this expected layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af0a4ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move saved xgboost model into fil model directory\n",
    "!mkdir -p model_repository/fil/1\n",
    "!cp xgboost.json model_repository/fil/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3413e6eb",
   "metadata": {},
   "source": [
    "And then finally we need to have configuration file `config.pbtxt` describing the model configuration for tree-based ML model so that FIL Backend in Triton can understand how to serve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754f8c5d",
   "metadata": {},
   "source": [
    "### Create Config File for FIL Backend Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c957cd",
   "metadata": {},
   "source": [
    "You can read about all generic Triton configuration options [here](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md) and about configuration options specific to the FIL backend [here](https://github.com/triton-inference-server/fil_backend#configuration), but we will focus on just a few of the most common and relevant options in this example. Below are general descriptions of these options:\n",
    "\n",
    "* **max_batch_size:** The maximum batch size that can be passed to this model. In general, the only limit on the size of batches passed to a FIL backend is the memory available with which to process them. \n",
    "* **input:** Options in this section tell Triton the number of features to expect for each input sample.\n",
    "* **output:** Options in this section tell Triton how many output values there will be for each sample. If the \"predict_proba\" option (described further on) is set to true, then a probability value will be returned for each class. Otherwise, a single value will be returned indicating the class predicted for the given sample.\n",
    "* **instance_group:** This determines how many instances of this model will be created and whether they will use the GPU or CPU.\n",
    "* **model_type:** A string indicating what format the model is in (\"xgboost_json\" in this example, but \"xgboost\", \"lightgbm\", and \"tl_checkpoint\" are valid formats as well).\n",
    "* **predict_proba:** If set to true, probability values will be returned for each class rather than just a class prediction.\n",
    "* **output_class:** True for classification models, false for regression models.\n",
    "* **threshold:** A score threshold for determining classification. When output_class is set to true, this must be provided, although it will not be used if predict_proba is also set to true.\n",
    "* **storage_type:** In general, using \"AUTO\" for this setting should meet most usecases. If \"AUTO\" storage is selected, FIL will load the model using either a sparse or dense representation based on the approximate size of the model. In some cases, you may want to explicitly set this to \"SPARSE\" in order to reduce the memory footprint of large models.\n",
    "\n",
    "Here we have 15 input features and 2 classes (FRAUD, NOT FRAUD) that we are doing classification for in our XGBoost Model. Based on this information, let's set up FIL Backend configuration file for our tree-based model for serving on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af7e054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "FIL_MODEL_DIR = \"./model_repository/fil\"\n",
    "\n",
    "# Maximum size in bytes for input and output arrays. If you are\n",
    "# using Triton 21.11 or higher, all memory allocations will make\n",
    "# use of Triton's memory pool, which has a default size of\n",
    "# 67_108_864 bytes\n",
    "MAX_MEMORY_BYTES = 60_000_000\n",
    "NUM_FEATURES = 15\n",
    "NUM_CLASSES = 2\n",
    "bytes_per_sample = (NUM_FEATURES + NUM_CLASSES) * 4\n",
    "max_batch_size = MAX_MEMORY_BYTES // bytes_per_sample\n",
    "\n",
    "IS_CLASSIFIER = True\n",
    "model_format = \"xgboost_json\"\n",
    "\n",
    "# Select deployment hardware (GPU or CPU)\n",
    "if USE_GPU:\n",
    "    instance_kind = \"KIND_GPU\"\n",
    "else:\n",
    "    instance_kind = \"KIND_CPU\"\n",
    "\n",
    "# whether the model is doing classification or regression\n",
    "if IS_CLASSIFIER:\n",
    "    classifier_string = \"true\"\n",
    "else:\n",
    "    classifier_string = \"false\"\n",
    "\n",
    "# whether to predict probabilites or not\n",
    "predict_proba = False\n",
    "\n",
    "if predict_proba:\n",
    "    predict_proba_string = \"true\"\n",
    "else:\n",
    "    predict_proba_string = \"false\"\n",
    "\n",
    "config_text = f\"\"\"backend: \"fil\"\n",
    "max_batch_size: {max_batch_size}\n",
    "input [                                 \n",
    " {{  \n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {NUM_FEATURES} ]                    \n",
    "  }} \n",
    "]\n",
    "output [\n",
    " {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: {instance_kind} }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"{model_format}\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"predict_proba\"\n",
    "    value: {{ string_value: \"{predict_proba_string}\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"{classifier_string}\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"threshold\"\n",
    "    value: {{ string_value: \"0.5\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"storage_type\"\n",
    "    value: {{ string_value: \"AUTO\" }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{}}\"\"\"\n",
    "\n",
    "config_path = os.path.join(FIL_MODEL_DIR, \"config.pbtxt\")\n",
    "with open(config_path, \"w\") as file_:\n",
    "    file_.write(config_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beceae2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up Inference Pipeline of Data Preprocessing Python Backend and FIL Backend using Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855b520",
   "metadata": {},
   "source": [
    "Now we are ready to set up the inference pipeline for data preprocessing and tree-based model inference using an [ensemble model](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/architecture.md#ensemble-models). An ensemble model represents a pipeline of one or more models and the connection of input and output tensors between those models. Here we use the ensemble model to build a pipeline of Data Preprocessing in Python backend followed by XGBoost in FIL Backend. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c04454",
   "metadata": {},
   "source": [
    "The expected layout for `ensemble` model directory is similar to the ones we showed above:\n",
    "\n",
    "<img src=\"./images/ensemble_model.png\" alt=\"ensemble-model\" width=\"200\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6274e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model version directory for ensemble model\n",
    "!mkdir -p model_repository/ensemble/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5061b0c4",
   "metadata": {},
   "source": [
    "We created the ensemble model's [config.pbtxt](model_repository/ensemble/config.pbtxt) following the guidance on [ensemble doc](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/architecture.md#ensemble-models). Importantly, we need to set up the ensemble scheduler in config.pbtxt which specifies the dataflow between models within the ensemble. The ensemble scheduler collects the output tensors in each step, provides them as input tensors for other steps according to the specification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125319fc",
   "metadata": {},
   "source": [
    "## Package model repository and upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d770f25d",
   "metadata": {},
   "source": [
    "Finally, we end up with the following model repository directory structure, containing a Python preprocessing model and its dependencies along with XGBoost FIL model, and the model ensemble.\n",
    "\n",
    "<img src=\"./images/model_repo.png\" alt=\"model-repo\" width=\"300\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c286c251",
   "metadata": {},
   "source": [
    "We will package this up as `model.tar.gz` for uploading it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3b29ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./preprocessing/\n",
      "./preprocessing/preprocessing_env.tar.gz\n",
      "./preprocessing/config.pbtxt\n",
      "./preprocessing/1/\n",
      "./preprocessing/1/model.py\n",
      "./preprocessing/1/label_encoders.pkl\n",
      "./fil/\n",
      "./fil/config.pbtxt\n",
      "./fil/1/\n",
      "./fil/1/xgboost.json\n",
      "./ensemble/\n",
      "./ensemble/config.pbtxt\n",
      "./ensemble/1/\n"
     ]
    }
   ],
   "source": [
    "!tar --exclude='.ipynb_checkpoints' -czvf model.tar.gz -C model_repository ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ec1e89d-e891-46ea-89d2-13e05c52dc97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'liftoff'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "proc=subprocess.Popen('cat /opt/ml/metadata/resource-metadata.json', shell=True, stdout=subprocess.PIPE, )\n",
    "studio_user_profile_output=json.loads(proc.communicate()[0].decode('utf-8'))['UserProfileName'] # retrieve current Studio User Profile name\n",
    "studio_user_profile_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21680877-a345-4ab4-bc8d-9f3b9c0dbc10",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "If you do not have access to the default bucket. You can upload the model tar ball to the bucket and prefix of your choice using the following code:\n",
    "\n",
    "```\n",
    "model_uri=\"s3://<YourBucketName>/<YourPrefix>/model.tar.gz\"\n",
    "\n",
    "!aws s3 cp model.tar.gz \"$model_uri\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7eacc699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-171503325295/liftoff/model.tar.gz'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_uri = sagemaker_session.upload_data(path=\"model.tar.gz\", key_prefix=studio_user_profile_output) # This method will upload the model tar ball to the SageMaker default bucket for the account in a prefix named as the User Profile for this Studio User. \n",
    "model_uri\n",
    "\n",
    "#model_uri=\"s3://<YourBucketName>/<YourPrefix>/model.tar.gz\"\n",
    "\n",
    "#!aws s3 cp model.tar.gz \"$model_uri\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2ff3c",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294a69cb",
   "metadata": {},
   "source": [
    "We start off by creating a SageMaker model from the model repository we uploaded to S3 in the previous step.\n",
    "\n",
    "In this step we also provide an additional Environment Variable `SAGEMAKER_TRITON_DEFAULT_MODEL_NAME` which specifies the name of the model to be loaded by Triton. **The value of this key should match the folder name in the model package uploaded to S3.** This variable is optional in case of a single model. In case of ensemble models, this **key has to be specified** for Triton to startup in SageMaker.\n",
    "\n",
    "Additionally, customers can set `SAGEMAKER_TRITON_BUFFER_MANAGER_THREAD_COUNT` and `SAGEMAKER_TRITON_THREAD_COUNT` for optimizing the thread counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb3c04ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-east-1:171503325295:model/triton-fil-ensemble-2023-01-21-04-50-51\n"
     ]
    }
   ],
   "source": [
    "sm_model_name = \"triton-fil-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"ensemble\"},\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9ac31",
   "metadata": {},
   "source": [
    "Using the model above, we create an endpoint configuration where we can specify the type and number of instances we want in the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0be165b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:171503325295:endpoint-config/triton-fil-ensemble-2023-01-21-04-50-52\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = \"triton-fil-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.xlarge\",\n",
    "            #\"InstanceType\": \"ml.g4dn.4xlarge\",\n",
    "            #\"InstanceType\": \"ml.g5.xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e132d08",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new SageMaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8e48705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:171503325295:endpoint/triton-fil-ensemble-2023-01-21-04-50-52\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"triton-fil-ensemble-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "635fd26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for endpoint to create...\n",
      "Endpoint Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-1:171503325295:endpoint/triton-fil-ensemble-2023-01-21-04-50-52\n"
     ]
    }
   ],
   "source": [
    "waiter = sm.get_waiter(\"endpoint_in_service\")\n",
    "print(\"Waiting for endpoint to create...\")\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "print(f\"Endpoint Status: {resp['EndpointStatus']}\")\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c2761",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4be74a",
   "metadata": {},
   "source": [
    "Once we have the endpoint running we can use some sample raw data to do an inference using json as the payload format. For the inference request format, Triton uses the KFServing community standard [inference protocols.](https://github.com/triton-inference-server/server/blob/main/docs/protocol/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96fb73bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Card</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Time</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Use Chip</th>\n",
       "      <th>Merchant Name</th>\n",
       "      <th>Merchant City</th>\n",
       "      <th>Merchant State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Errors?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>13:57</td>\n",
       "      <td>$7.18</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>2027553650310142703</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>80918.0</td>\n",
       "      <td>5541</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>940</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>21:16</td>\n",
       "      <td>$37.47</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>7069584154815291371</td>\n",
       "      <td>Reading</td>\n",
       "      <td>MA</td>\n",
       "      <td>1867.0</td>\n",
       "      <td>5812</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1846</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>13:32</td>\n",
       "      <td>$10.62</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>5474320255037684877</td>\n",
       "      <td>Saint Louis</td>\n",
       "      <td>MO</td>\n",
       "      <td>63146.0</td>\n",
       "      <td>5912</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1398</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>07:35</td>\n",
       "      <td>$24.52</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-6738340320657348028</td>\n",
       "      <td>Georgetown</td>\n",
       "      <td>IL</td>\n",
       "      <td>61846.0</td>\n",
       "      <td>7538</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1192</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>14:46</td>\n",
       "      <td>$95.30</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-1662162349688630531</td>\n",
       "      <td>Newport News</td>\n",
       "      <td>VA</td>\n",
       "      <td>23605.0</td>\n",
       "      <td>5541</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User  Card  Year  Month  Day   Time  Amount           Use Chip  \\\n",
       "0   275     0  2000      7   25  13:57   $7.18  Swipe Transaction   \n",
       "1   940     0  2016     10   27  21:16  $37.47  Swipe Transaction   \n",
       "2  1846     0  2013      9    2  13:32  $10.62  Swipe Transaction   \n",
       "3  1398     0  2014      9   20  07:35  $24.52  Swipe Transaction   \n",
       "4  1192     2  2013     12   10  14:46  $95.30  Swipe Transaction   \n",
       "\n",
       "         Merchant Name     Merchant City Merchant State      Zip   MCC  \\\n",
       "0  2027553650310142703  Colorado Springs             CO  80918.0  5541   \n",
       "1  7069584154815291371           Reading             MA   1867.0  5812   \n",
       "2  5474320255037684877       Saint Louis             MO  63146.0  5912   \n",
       "3 -6738340320657348028        Georgetown             IL  61846.0  7538   \n",
       "4 -1662162349688630531      Newport News             VA  23605.0  5541   \n",
       "\n",
       "   Errors?  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_infer = pd.read_csv(\"data_infer.csv\")\n",
    "data_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0749cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "STR_COLUMNS = [\n",
    "    \"Time\",\n",
    "    \"Amount\",\n",
    "    \"Zip\",\n",
    "    \"MCC\",\n",
    "    \"Merchant Name\",\n",
    "    \"Use Chip\",\n",
    "    \"Merchant City\",\n",
    "    \"Merchant State\",\n",
    "    \"Errors?\",\n",
    "]\n",
    "\n",
    "batch_size = len(data_infer)\n",
    "\n",
    "payload = {}\n",
    "payload[\"inputs\"] = []\n",
    "data_dict = {}\n",
    "for col_name in data_infer.columns:\n",
    "    data_dict[col_name] = {}\n",
    "    data_dict[col_name][\"name\"] = col_name\n",
    "    if col_name in STR_COLUMNS:\n",
    "        data_dict[col_name][\"data\"] = data_infer[col_name].astype(str).tolist()\n",
    "        data_dict[col_name][\"datatype\"] = \"BYTES\"\n",
    "    else:\n",
    "        data_dict[col_name][\"data\"] = data_infer[col_name].astype(\"float32\").tolist()\n",
    "        data_dict[col_name][\"datatype\"] = \"FP32\"\n",
    "    data_dict[col_name][\"shape\"] = [batch_size, 1]\n",
    "    payload[\"inputs\"].append(data_dict[col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94ca03cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOT FRAUD', 'NOT FRAUD', 'NOT FRAUD', 'NOT FRAUD', 'NOT FRAUD']\n"
     ]
    }
   ],
   "source": [
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "response_body = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "predictions = response_body[\"outputs\"][0][\"data\"]\n",
    "\n",
    "CLASS_LABELS = [\"NOT FRAUD\", \"FRAUD\"]\n",
    "predictions = [CLASS_LABELS[int(idx)] for idx in predictions]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40cfcae",
   "metadata": {},
   "source": [
    "### Binary + Json Payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69eaf44",
   "metadata": {},
   "source": [
    "We can also use binary+json as the payload format to get better performance for the inference call. The specification of this format is provided [here](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md).\n",
    "\n",
    "**Note:** With the `binary+json` format, we have to specify the length of the request metadata in the header to allow Triton to correctly parse the binary payload. This is done using a custom Content-Type header `application/vnd.sagemaker-triton.binary+json;json-header-size={}`.\n",
    "\n",
    "Please note, this is different from using `Inference-Header-Content-Length` header on a stand-alone Triton server since custom headers are not allowed in SageMaker.\n",
    "\n",
    "The [tritonclient](https://github.com/triton-inference-server/client) package provides utility methods to generate the payload without having to know the details of the specification. We'll use the following methods to convert our inference request into a binary format which provides lower latencies for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f38ef326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.http as httpclient\n",
    "\n",
    "\n",
    "def get_sample_data_binary(data, output_name):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    batch_size = len(data)\n",
    "    for col_name in data.columns:\n",
    "        if col_name in STR_COLUMNS:\n",
    "            np_data = np.expand_dims(data[col_name], axis=1).astype(\"object\")\n",
    "            infer_input = httpclient.InferInput(col_name, [batch_size, 1], \"BYTES\")\n",
    "        else:\n",
    "            np_data = np.expand_dims(data[col_name], axis=1).astype(\"float32\")\n",
    "            infer_input = httpclient.InferInput(col_name, [batch_size, 1], \"FP32\")\n",
    "        infer_input.set_data_from_numpy(np_data, binary_data=True)\n",
    "        inputs.append(infer_input)\n",
    "    outputs.append(httpclient.InferRequestedOutput(output_name, binary_data=True))\n",
    "    request_body, header_length = httpclient.InferenceServerClient.generate_request_body(\n",
    "        inputs, outputs=outputs\n",
    "    )\n",
    "    return request_body, header_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1547f43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOT FRAUD', 'NOT FRAUD', 'NOT FRAUD', 'NOT FRAUD', 'NOT FRAUD']\n"
     ]
    }
   ],
   "source": [
    "output_name = \"predictions\"\n",
    "request_body, header_length = get_sample_data_binary(data_infer, output_name)\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/vnd.sagemaker-triton.binary+json;json-header-size={}\".format(\n",
    "        header_length\n",
    "    ),\n",
    "    Body=request_body,\n",
    ")\n",
    "\n",
    "# Parse json header size length from the response\n",
    "header_length_prefix = \"application/vnd.sagemaker-triton.binary+json;json-header-size=\"\n",
    "header_length_str = response[\"ContentType\"][len(header_length_prefix) :]\n",
    "\n",
    "# Read response body\n",
    "result = httpclient.InferenceServerClient.parse_response_body(\n",
    "    response[\"Body\"].read(), header_length=int(header_length_str)\n",
    ")\n",
    "predictions = result.as_numpy(output_name)\n",
    "CLASS_LABELS = [\"NOT FRAUD\", \"FRAUD\"]\n",
    "predictions = [CLASS_LABELS[int(idx)] for idx in predictions]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7154ecdd",
   "metadata": {},
   "source": [
    "## Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03f60fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'c5bb3c9f-39ad-4986-8392-f497d0b3f656',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'c5bb3c9f-39ad-4986-8392-f497d0b3f656',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Sat, 21 Jan 2023 04:56:09 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_model(ModelName=sm_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b870fa3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this Lab1 leveraging Triton to create an ensemble to do Python preprocessing and used the XGBoost model to show how fraud can be detected using Triton and its corresponding Python and FIL backends. This example can further be used as a guide to create your own ensembles leveraging the other backends that Triton provides solving a wide variety of use cases that you may have that require scale and performance while using hardware for acceleration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81820bd-0320-43c7-94e9-638b0f3987a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
